2026-02-08 02:03:17,922 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.0.100:5000
2026-02-08 02:03:17,922 - INFO - [33mPress CTRL+C to quit[0m
2026-02-08 02:03:26,481 - INFO - User input received: hi
2026-02-08 02:03:26,481 - INFO - Loading vector store for context
2026-02-08 02:03:26,481 - INFO - Initializing HuggingFaceEmbeddings model...
2026-02-08 02:03:26,483 - INFO - Use pytorch device_name: cpu
2026-02-08 02:03:26,483 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2026-02-08 02:03:27,041 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:03:27,057 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-08 02:03:27,315 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:03:27,328 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-08 02:03:27,584 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:03:27,598 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-08 02:03:27,850 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:03:27,863 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md "HTTP/1.1 200 OK"
2026-02-08 02:03:28,114 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:03:28,127 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-08 02:03:28,373 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:03:28,386 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json "HTTP/1.1 200 OK"
2026-02-08 02:03:28,636 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json "HTTP/1.1 404 Not Found"
2026-02-08 02:03:28,888 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:03:28,904 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-08 02:03:29,563 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:03:29,581 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-08 02:03:29,832 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:03:29,846 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json "HTTP/1.1 200 OK"
2026-02-08 02:03:30,104 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false "HTTP/1.1 404 Not Found"
2026-02-08 02:03:30,364 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=true&expand=false "HTTP/1.1 200 OK"
2026-02-08 02:03:30,677 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:03:30,690 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json "HTTP/1.1 200 OK"
2026-02-08 02:03:30,958 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2 "HTTP/1.1 200 OK"
2026-02-08 02:03:30,963 - INFO - HuggingFaceEmbeddings model initialized successfully.
2026-02-08 02:03:30,963 - INFO - Loading existing FAISS vector store...
2026-02-08 02:03:30,965 - INFO - Loading faiss with AVX2 support.
2026-02-08 02:03:30,987 - INFO - Successfully loaded faiss with AVX2 support.
2026-02-08 02:03:31,029 - INFO - Using HF model repo id: Qwen/Qwen2.5-0.5B-Instruct
2026-02-08 02:03:31,029 - INFO - HF token present: True
2026-02-08 02:03:31,029 - INFO - Loading LLM from HuggingFace
2026-02-08 02:03:31,130 - INFO - LLM loaded sucesfully...
2026-02-08 02:03:31,131 - INFO - Sucesfully created the QA chain
2026-02-08 02:03:31,131 - INFO - QAChain type: <class 'langchain_classic.chains.retrieval_qa.base.RetrievalQA'>
2026-02-08 02:03:31,131 - INFO - Input keys: ['query']
2026-02-08 02:03:31,131 - INFO - Output keys: ['result']
2026-02-08 02:03:31,406 - INFO - HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen2.5-0.5B-Instruct?expand=inferenceProviderMapping "HTTP/1.1 200 OK"
2026-02-08 02:03:31,406 - ERROR - Error while running QA chain
Traceback (most recent call last):
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\applications.py", line 41, in index
    response = qa_chain.invoke({"query" : user_input})
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 167, in invoke
    self._call(inputs, run_manager=run_manager)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\retrieval_qa\base.py", line 154, in _call
    answer = self.combine_documents_chain.run(
        input_documents=docs,
        question=question,
        callbacks=_run_manager.get_child(),
    )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\_api\deprecation.py", line 205, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 637, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\_api\deprecation.py", line 205, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 413, in __call__
    return self.invoke(
           ~~~~~~~~~~~^
        inputs,
        ^^^^^^^
    ...<2 lines>...
        include_run_info=include_run_info,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 167, in invoke
    self._call(inputs, run_manager=run_manager)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\combine_documents\base.py", line 141, in _call
    output, extra_return_dict = self.combine_docs(
                                ~~~~~~~~~~~~~~~~~^
        docs,
        ^^^^^
        callbacks=_run_manager.get_child(),
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        **other_keys,
        ^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\combine_documents\stuff.py", line 266, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\llm.py", line 315, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\_api\deprecation.py", line 205, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 413, in __call__
    return self.invoke(
           ~~~~~~~~~~~^
        inputs,
        ^^^^^^^
    ...<2 lines>...
        include_run_info=include_run_info,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 167, in invoke
    self._call(inputs, run_manager=run_manager)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\llm.py", line 117, in _call
    response = self.generate([inputs], run_manager=run_manager)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\llm.py", line 129, in generate
    return self.llm.generate_prompt(
           ~~~~~~~~~~~~~~~~~~~~~~~~^
        prompts,
        ^^^^^^^^
    ...<2 lines>...
        **self.llm_kwargs,
        ^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 789, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 1011, in generate
    return self._generate_helper(
           ~~~~~~~~~~~~~~~~~~~~~^
        prompts,
        ^^^^^^^^
    ...<3 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 815, in _generate_helper
    self._generate(
    ~~~~~~~~~~~~~~^
        prompts,
        ^^^^^^^^
    ...<3 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 1505, in _generate
    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_huggingface\llms\huggingface_endpoint.py", line 341, in _call
    response_text = self.client.text_generation(
        prompt=prompt,
        model=self.model,
        **invocation_params,
    )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_client.py", line 2386, in text_generation
    provider_helper = get_provider_helper(self.provider, task="text-generation", model=model_id)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_providers\__init__.py", line 253, in get_provider_helper
    provider = next(iter(provider_mapping)).provider
               ~~~~^^^^^^^^^^^^^^^^^^^^^^^^
StopIteration
2026-02-08 02:03:31,420 - INFO - 127.0.0.1 - - [08/Feb/2026 02:03:31] "POST / HTTP/1.1" 200 -
2026-02-08 02:03:35,899 - INFO - User input received: hi
2026-02-08 02:03:35,899 - INFO - Loading vector store for context
2026-02-08 02:03:35,899 - INFO - Initializing HuggingFaceEmbeddings model...
2026-02-08 02:03:35,900 - INFO - Use pytorch device_name: cpu
2026-02-08 02:03:35,900 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2026-02-08 02:03:36,209 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:03:36,225 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-08 02:03:36,553 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:03:36,569 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-08 02:03:37,102 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:03:37,115 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-08 02:03:37,383 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:03:37,436 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md "HTTP/1.1 200 OK"
2026-02-08 02:03:37,789 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:03:37,802 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-08 02:03:38,058 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:03:38,071 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json "HTTP/1.1 200 OK"
2026-02-08 02:03:38,338 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json "HTTP/1.1 404 Not Found"
2026-02-08 02:03:38,588 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:03:38,603 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-08 02:03:38,943 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:03:38,958 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-08 02:03:39,216 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:03:39,231 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json "HTTP/1.1 200 OK"
2026-02-08 02:03:39,484 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false "HTTP/1.1 404 Not Found"
2026-02-08 02:03:39,742 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=true&expand=false "HTTP/1.1 200 OK"
2026-02-08 02:03:40,096 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:03:40,109 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json "HTTP/1.1 200 OK"
2026-02-08 02:03:40,377 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2 "HTTP/1.1 200 OK"
2026-02-08 02:03:40,379 - INFO - HuggingFaceEmbeddings model initialized successfully.
2026-02-08 02:03:40,379 - INFO - Loading existing FAISS vector store...
2026-02-08 02:03:40,525 - INFO - Using HF model repo id: Qwen/Qwen2.5-0.5B-Instruct
2026-02-08 02:03:40,525 - INFO - HF token present: True
2026-02-08 02:03:40,525 - INFO - Loading LLM from HuggingFace
2026-02-08 02:03:40,525 - INFO - LLM loaded sucesfully...
2026-02-08 02:03:40,526 - INFO - Sucesfully created the QA chain
2026-02-08 02:03:40,526 - INFO - QAChain type: <class 'langchain_classic.chains.retrieval_qa.base.RetrievalQA'>
2026-02-08 02:03:40,526 - INFO - Input keys: ['query']
2026-02-08 02:03:40,526 - INFO - Output keys: ['result']
2026-02-08 02:03:40,539 - ERROR - Error while running QA chain
Traceback (most recent call last):
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\applications.py", line 41, in index
    response = qa_chain.invoke({"query" : user_input})
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 167, in invoke
    self._call(inputs, run_manager=run_manager)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\retrieval_qa\base.py", line 154, in _call
    answer = self.combine_documents_chain.run(
        input_documents=docs,
        question=question,
        callbacks=_run_manager.get_child(),
    )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\_api\deprecation.py", line 205, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 637, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\_api\deprecation.py", line 205, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 413, in __call__
    return self.invoke(
           ~~~~~~~~~~~^
        inputs,
        ^^^^^^^
    ...<2 lines>...
        include_run_info=include_run_info,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 167, in invoke
    self._call(inputs, run_manager=run_manager)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\combine_documents\base.py", line 141, in _call
    output, extra_return_dict = self.combine_docs(
                                ~~~~~~~~~~~~~~~~~^
        docs,
        ^^^^^
        callbacks=_run_manager.get_child(),
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        **other_keys,
        ^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\combine_documents\stuff.py", line 266, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\llm.py", line 315, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\_api\deprecation.py", line 205, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 413, in __call__
    return self.invoke(
           ~~~~~~~~~~~^
        inputs,
        ^^^^^^^
    ...<2 lines>...
        include_run_info=include_run_info,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 167, in invoke
    self._call(inputs, run_manager=run_manager)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\llm.py", line 117, in _call
    response = self.generate([inputs], run_manager=run_manager)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\llm.py", line 129, in generate
    return self.llm.generate_prompt(
           ~~~~~~~~~~~~~~~~~~~~~~~~^
        prompts,
        ^^^^^^^^
    ...<2 lines>...
        **self.llm_kwargs,
        ^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 789, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 1011, in generate
    return self._generate_helper(
           ~~~~~~~~~~~~~~~~~~~~~^
        prompts,
        ^^^^^^^^
    ...<3 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 815, in _generate_helper
    self._generate(
    ~~~~~~~~~~~~~~^
        prompts,
        ^^^^^^^^
    ...<3 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 1505, in _generate
    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_huggingface\llms\huggingface_endpoint.py", line 341, in _call
    response_text = self.client.text_generation(
        prompt=prompt,
        model=self.model,
        **invocation_params,
    )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_client.py", line 2386, in text_generation
    provider_helper = get_provider_helper(self.provider, task="text-generation", model=model_id)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_providers\__init__.py", line 253, in get_provider_helper
    provider = next(iter(provider_mapping)).provider
               ~~~~^^^^^^^^^^^^^^^^^^^^^^^^
StopIteration
2026-02-08 02:03:40,546 - INFO - 127.0.0.1 - - [08/Feb/2026 02:03:40] "POST / HTTP/1.1" 200 -
2026-02-08 02:05:39,059 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.0.100:5000
2026-02-08 02:05:39,059 - INFO - [33mPress CTRL+C to quit[0m
2026-02-08 02:05:50,257 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.0.100:5000
2026-02-08 02:05:50,257 - INFO - [33mPress CTRL+C to quit[0m
2026-02-08 02:06:09,833 - INFO - User input received: hi
2026-02-08 02:06:09,833 - INFO - Loading vector store for context
2026-02-08 02:06:09,833 - INFO - Initializing HuggingFaceEmbeddings model...
2026-02-08 02:06:09,835 - INFO - Use pytorch device_name: cpu
2026-02-08 02:06:09,835 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2026-02-08 02:06:10,354 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:06:10,368 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-08 02:06:10,617 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:06:10,632 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-08 02:06:10,884 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:06:10,898 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-08 02:06:11,151 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:06:11,167 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md "HTTP/1.1 200 OK"
2026-02-08 02:06:11,423 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:06:11,436 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-08 02:06:11,685 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:06:11,701 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json "HTTP/1.1 200 OK"
2026-02-08 02:06:11,960 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json "HTTP/1.1 404 Not Found"
2026-02-08 02:06:12,216 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:06:12,234 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-08 02:06:12,633 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:06:12,646 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-08 02:06:12,904 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:06:12,917 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json "HTTP/1.1 200 OK"
2026-02-08 02:06:13,171 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false "HTTP/1.1 404 Not Found"
2026-02-08 02:06:13,429 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=true&expand=false "HTTP/1.1 200 OK"
2026-02-08 02:06:13,758 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:06:13,775 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json "HTTP/1.1 200 OK"
2026-02-08 02:06:14,046 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2 "HTTP/1.1 200 OK"
2026-02-08 02:06:14,050 - INFO - HuggingFaceEmbeddings model initialized successfully.
2026-02-08 02:06:14,050 - INFO - Loading existing FAISS vector store...
2026-02-08 02:06:14,052 - INFO - Loading faiss with AVX2 support.
2026-02-08 02:06:14,074 - INFO - Successfully loaded faiss with AVX2 support.
2026-02-08 02:06:14,113 - INFO - Using HF model repo id: mistralai/Mistral-7B-Instruct-v0.3
2026-02-08 02:06:14,113 - INFO - HF token present: True
2026-02-08 02:06:14,113 - INFO - Loading LLM from HuggingFace
2026-02-08 02:06:14,208 - INFO - LLM loaded sucesfully...
2026-02-08 02:06:14,208 - INFO - Sucesfully created the QA chain
2026-02-08 02:06:14,209 - INFO - QAChain type: <class 'langchain_classic.chains.retrieval_qa.base.RetrievalQA'>
2026-02-08 02:06:14,209 - INFO - Input keys: ['query']
2026-02-08 02:06:14,209 - INFO - Output keys: ['result']
2026-02-08 02:06:14,471 - INFO - HTTP Request: GET https://huggingface.co/api/models/mistralai/Mistral-7B-Instruct-v0.3?expand=inferenceProviderMapping "HTTP/1.1 200 OK"
2026-02-08 02:06:14,472 - ERROR - Error while running QA chain
Traceback (most recent call last):
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\applications.py", line 41, in index
    response = qa_chain.invoke({"query" : user_input})
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 167, in invoke
    self._call(inputs, run_manager=run_manager)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\retrieval_qa\base.py", line 154, in _call
    answer = self.combine_documents_chain.run(
        input_documents=docs,
        question=question,
        callbacks=_run_manager.get_child(),
    )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\_api\deprecation.py", line 205, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 637, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\_api\deprecation.py", line 205, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 413, in __call__
    return self.invoke(
           ~~~~~~~~~~~^
        inputs,
        ^^^^^^^
    ...<2 lines>...
        include_run_info=include_run_info,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 167, in invoke
    self._call(inputs, run_manager=run_manager)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\combine_documents\base.py", line 141, in _call
    output, extra_return_dict = self.combine_docs(
                                ~~~~~~~~~~~~~~~~~^
        docs,
        ^^^^^
        callbacks=_run_manager.get_child(),
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        **other_keys,
        ^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\combine_documents\stuff.py", line 266, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\llm.py", line 315, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\_api\deprecation.py", line 205, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 413, in __call__
    return self.invoke(
           ~~~~~~~~~~~^
        inputs,
        ^^^^^^^
    ...<2 lines>...
        include_run_info=include_run_info,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 167, in invoke
    self._call(inputs, run_manager=run_manager)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\llm.py", line 117, in _call
    response = self.generate([inputs], run_manager=run_manager)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\llm.py", line 129, in generate
    return self.llm.generate_prompt(
           ~~~~~~~~~~~~~~~~~~~~~~~~^
        prompts,
        ^^^^^^^^
    ...<2 lines>...
        **self.llm_kwargs,
        ^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 789, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 1011, in generate
    return self._generate_helper(
           ~~~~~~~~~~~~~~~~~~~~~^
        prompts,
        ^^^^^^^^
    ...<3 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 815, in _generate_helper
    self._generate(
    ~~~~~~~~~~~~~~^
        prompts,
        ^^^^^^^^
    ...<3 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 1505, in _generate
    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_huggingface\llms\huggingface_endpoint.py", line 341, in _call
    response_text = self.client.text_generation(
        prompt=prompt,
        model=self.model,
        **invocation_params,
    )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_client.py", line 2387, in text_generation
    request_parameters = provider_helper.prepare_request(
        inputs=prompt,
    ...<4 lines>...
        api_key=self.token,
    )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_providers\_common.py", line 95, in prepare_request
    provider_mapping_info = self._prepare_mapping_info(model)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_providers\_common.py", line 173, in _prepare_mapping_info
    raise ValueError(
    ...<2 lines>...
    )
ValueError: Model mistralai/Mistral-7B-Instruct-v0.3 is not supported for task text-generation and provider together. Supported task: conversational.
2026-02-08 02:06:14,485 - INFO - 127.0.0.1 - - [08/Feb/2026 02:06:14] "POST / HTTP/1.1" 200 -
2026-02-08 02:08:12,927 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.0.100:5000
2026-02-08 02:08:12,928 - INFO - [33mPress CTRL+C to quit[0m
2026-02-08 02:08:14,321 - INFO - User input received: hi
2026-02-08 02:08:14,321 - INFO - Loading vector store for context
2026-02-08 02:08:14,321 - INFO - Initializing HuggingFaceEmbeddings model...
2026-02-08 02:08:14,322 - INFO - Use pytorch device_name: cpu
2026-02-08 02:08:14,322 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2026-02-08 02:08:14,952 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:08:14,969 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-08 02:08:15,296 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:08:15,311 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-08 02:08:15,566 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:08:15,578 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-08 02:08:15,836 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:08:15,851 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md "HTTP/1.1 200 OK"
2026-02-08 02:08:16,118 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:08:16,132 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-08 02:08:16,455 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:08:16,469 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json "HTTP/1.1 200 OK"
2026-02-08 02:08:16,729 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json "HTTP/1.1 404 Not Found"
2026-02-08 02:08:16,995 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:08:17,012 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-08 02:08:17,411 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:08:17,426 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-08 02:08:17,753 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:08:17,770 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json "HTTP/1.1 200 OK"
2026-02-08 02:08:18,292 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false "HTTP/1.1 404 Not Found"
2026-02-08 02:08:18,568 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=true&expand=false "HTTP/1.1 200 OK"
2026-02-08 02:08:18,893 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:08:18,908 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json "HTTP/1.1 200 OK"
2026-02-08 02:08:19,174 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2 "HTTP/1.1 200 OK"
2026-02-08 02:08:19,177 - INFO - HuggingFaceEmbeddings model initialized successfully.
2026-02-08 02:08:19,177 - INFO - Loading existing FAISS vector store...
2026-02-08 02:08:19,179 - INFO - Loading faiss with AVX2 support.
2026-02-08 02:08:19,200 - INFO - Successfully loaded faiss with AVX2 support.
2026-02-08 02:08:19,237 - INFO - Using HF model repo id: mistralai/Mistral-7B-Instruct-v0.3
2026-02-08 02:08:19,237 - INFO - HF token present: True
2026-02-08 02:08:19,237 - INFO - Loading LLM from HuggingFace
2026-02-08 02:08:19,437 - INFO - LLM loaded sucesfully...
2026-02-08 02:08:19,439 - INFO - Sucesfully created the QA chain
2026-02-08 02:08:19,439 - INFO - QAChain type: <class 'langchain_classic.chains.retrieval_qa.base.RetrievalQA'>
2026-02-08 02:08:19,439 - INFO - Input keys: ['query']
2026-02-08 02:08:19,439 - INFO - Output keys: ['result']
2026-02-08 02:08:19,891 - INFO - HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 400 Bad Request"
2026-02-08 02:08:19,893 - ERROR - Error while running QA chain
Traceback (most recent call last):
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\utils\_http.py", line 657, in hf_raise_for_status
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://router.huggingface.co/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\applications.py", line 41, in index
    response = qa_chain.invoke({"query" : user_input})
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 167, in invoke
    self._call(inputs, run_manager=run_manager)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\retrieval_qa\base.py", line 154, in _call
    answer = self.combine_documents_chain.run(
        input_documents=docs,
        question=question,
        callbacks=_run_manager.get_child(),
    )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\_api\deprecation.py", line 205, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 637, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\_api\deprecation.py", line 205, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 413, in __call__
    return self.invoke(
           ~~~~~~~~~~~^
        inputs,
        ^^^^^^^
    ...<2 lines>...
        include_run_info=include_run_info,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 167, in invoke
    self._call(inputs, run_manager=run_manager)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\combine_documents\base.py", line 141, in _call
    output, extra_return_dict = self.combine_docs(
                                ~~~~~~~~~~~~~~~~~^
        docs,
        ^^^^^
        callbacks=_run_manager.get_child(),
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        **other_keys,
        ^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\combine_documents\stuff.py", line 266, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\llm.py", line 315, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\_api\deprecation.py", line 205, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 413, in __call__
    return self.invoke(
           ~~~~~~~~~~~^
        inputs,
        ^^^^^^^
    ...<2 lines>...
        include_run_info=include_run_info,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 167, in invoke
    self._call(inputs, run_manager=run_manager)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\llm.py", line 117, in _call
    response = self.generate([inputs], run_manager=run_manager)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\llm.py", line 129, in generate
    return self.llm.generate_prompt(
           ~~~~~~~~~~~~~~~~~~~~~~~~^
        prompts,
        ^^^^^^^^
    ...<2 lines>...
        **self.llm_kwargs,
        ^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\chat_models.py", line 931, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_huggingface\chat_models\huggingface.py", line 750, in _generate
    answer = self.llm.client.chat_completion(messages=message_dicts, **params)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_client.py", line 933, in chat_completion
    data = self._inner_post(request_parameters, stream=stream)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_client.py", line 286, in _inner_post
    hf_raise_for_status(response)
    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\utils\_http.py", line 716, in hf_raise_for_status
    raise _format(BadRequestError, message, response) from e
huggingface_hub.errors.BadRequestError: (Request ID: Root=1-69879bb2-10ba311562c5b6a4250a7d84;ff66a087-8246-4c42-a3ad-f00c2a4b31cb)

Bad request:
{'message': "The requested model 'mistralai/Mistral-7B-Instruct-v0.3' is not a chat model.", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'}
2026-02-08 02:08:19,927 - INFO - 127.0.0.1 - - [08/Feb/2026 02:08:19] "POST / HTTP/1.1" 200 -
2026-02-08 02:13:06,023 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.0.100:5000
2026-02-08 02:13:06,024 - INFO - [33mPress CTRL+C to quit[0m
2026-02-08 02:13:08,228 - INFO - User input received: hi
2026-02-08 02:13:08,229 - INFO - Loading vector store for context
2026-02-08 02:13:08,229 - INFO - Initializing HuggingFaceEmbeddings model...
2026-02-08 02:13:08,230 - INFO - Use pytorch device_name: cpu
2026-02-08 02:13:08,230 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2026-02-08 02:13:08,737 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:13:08,770 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-08 02:13:09,031 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:13:09,049 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-08 02:13:09,300 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:13:09,313 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-08 02:13:09,562 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:13:09,577 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md "HTTP/1.1 200 OK"
2026-02-08 02:13:09,831 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:13:09,853 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-08 02:13:10,110 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:13:10,123 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json "HTTP/1.1 200 OK"
2026-02-08 02:13:10,376 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json "HTTP/1.1 404 Not Found"
2026-02-08 02:13:10,665 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:13:10,681 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-08 02:13:11,047 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:13:11,061 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-08 02:13:11,310 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:13:11,324 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json "HTTP/1.1 200 OK"
2026-02-08 02:13:11,581 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false "HTTP/1.1 404 Not Found"
2026-02-08 02:13:11,841 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=true&expand=false "HTTP/1.1 200 OK"
2026-02-08 02:13:12,168 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:13:12,184 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json "HTTP/1.1 200 OK"
2026-02-08 02:13:12,454 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2 "HTTP/1.1 200 OK"
2026-02-08 02:13:12,470 - INFO - HuggingFaceEmbeddings model initialized successfully.
2026-02-08 02:13:12,471 - INFO - Loading existing FAISS vector store...
2026-02-08 02:13:12,476 - INFO - Loading faiss with AVX2 support.
2026-02-08 02:13:12,501 - INFO - Successfully loaded faiss with AVX2 support.
2026-02-08 02:13:12,538 - INFO - Using HF model repo id: mistralai/Mistral-7B-Instruct-v0.3
2026-02-08 02:13:12,538 - INFO - HF token present: True
2026-02-08 02:13:12,538 - INFO - Loading LLM from HuggingFace
2026-02-08 02:13:12,883 - INFO - HTTP Request: GET https://huggingface.co/api/models/mistralai/Mistral-7B-Instruct-v0.3?expand=inferenceProviderMapping "HTTP/1.1 200 OK"
2026-02-08 02:13:12,883 - WARNING - text-generation probe failed; trying conversational. Reason: Model mistralai/Mistral-7B-Instruct-v0.3 is not supported for task text-generation and provider together. Supported task: conversational.
2026-02-08 02:13:13,186 - INFO - HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 400 Bad Request"
2026-02-08 02:13:13,188 - ERROR - Failed to load a llm | Error: Hugging Face model/provider does not support either text-generation or conversational tasks. Try a different model, set HF_LLM_TASK='text-generation', or choose a provider that supports the task. | Error: (Request ID: Root=1-69879cd7-22e8e4a063dfa75d1feab252;2e15c86d-b154-4663-b3fc-a25ef71ddcf8)

Bad request:
{'message': "The requested model 'mistralai/Mistral-7B-Instruct-v0.3' is not a chat model.", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'} | File: D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\llm.py | Line: 72 | File: D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\llm.py | Line: 77
2026-02-08 02:13:13,188 - ERROR - Failed to make a QA chain
Traceback (most recent call last):
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\llm.py", line 63, in load_llm
    probe_text(llm)
    ~~~~~~~~~~^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\llm.py", line 44, in probe_text
    _ = text_llm.invoke("ping")
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 378, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 789, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 1011, in generate
    return self._generate_helper(
           ~~~~~~~~~~~~~~~~~~~~~^
        prompts,
        ^^^^^^^^
    ...<3 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 815, in _generate_helper
    self._generate(
    ~~~~~~~~~~~~~~^
        prompts,
        ^^^^^^^^
    ...<3 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 1505, in _generate
    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_huggingface\llms\huggingface_endpoint.py", line 341, in _call
    response_text = self.client.text_generation(
        prompt=prompt,
        model=self.model,
        **invocation_params,
    )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_client.py", line 2387, in text_generation
    request_parameters = provider_helper.prepare_request(
        inputs=prompt,
    ...<4 lines>...
        api_key=self.token,
    )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_providers\_common.py", line 95, in prepare_request
    provider_mapping_info = self._prepare_mapping_info(model)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_providers\_common.py", line 173, in _prepare_mapping_info
    raise ValueError(
    ...<2 lines>...
    )
ValueError: Model mistralai/Mistral-7B-Instruct-v0.3 is not supported for task text-generation and provider together. Supported task: conversational.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\retriever.py", line 42, in create_qa_chain
    llm = load_llm(huggingface_repo_id=HUGGINGFACE_REPO_ID , hf_token=HF_TOKEN )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\llm.py", line 77, in load_llm
    raise CustomException(
    ...<3 lines>...
    ) from text_err
app.common.custom_exception.CustomException: Hugging Face model/provider does not support either text-generation or conversational tasks. Try a different model, set HF_LLM_TASK='text-generation', or choose a provider that supports the task. | Error: (Request ID: Root=1-69879cd7-22e8e4a063dfa75d1feab252;2e15c86d-b154-4663-b3fc-a25ef71ddcf8)

Bad request:
{'message': "The requested model 'mistralai/Mistral-7B-Instruct-v0.3' is not a chat model.", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'} | File: D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\llm.py | Line: 72
2026-02-08 02:13:13,194 - ERROR - Error while running QA chain
Traceback (most recent call last):
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\llm.py", line 63, in load_llm
    probe_text(llm)
    ~~~~~~~~~~^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\llm.py", line 44, in probe_text
    _ = text_llm.invoke("ping")
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 378, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 789, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 1011, in generate
    return self._generate_helper(
           ~~~~~~~~~~~~~~~~~~~~~^
        prompts,
        ^^^^^^^^
    ...<3 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 815, in _generate_helper
    self._generate(
    ~~~~~~~~~~~~~~^
        prompts,
        ^^^^^^^^
    ...<3 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 1505, in _generate
    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_huggingface\llms\huggingface_endpoint.py", line 341, in _call
    response_text = self.client.text_generation(
        prompt=prompt,
        model=self.model,
        **invocation_params,
    )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_client.py", line 2387, in text_generation
    request_parameters = provider_helper.prepare_request(
        inputs=prompt,
    ...<4 lines>...
        api_key=self.token,
    )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_providers\_common.py", line 95, in prepare_request
    provider_mapping_info = self._prepare_mapping_info(model)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_providers\_common.py", line 173, in _prepare_mapping_info
    raise ValueError(
    ...<2 lines>...
    )
ValueError: Model mistralai/Mistral-7B-Instruct-v0.3 is not supported for task text-generation and provider together. Supported task: conversational.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\applications.py", line 36, in index
    qa_chain = create_qa_chain()
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\retriever.py", line 42, in create_qa_chain
    llm = load_llm(huggingface_repo_id=HUGGINGFACE_REPO_ID , hf_token=HF_TOKEN )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\llm.py", line 77, in load_llm
    raise CustomException(
    ...<3 lines>...
    ) from text_err
app.common.custom_exception.CustomException: Hugging Face model/provider does not support either text-generation or conversational tasks. Try a different model, set HF_LLM_TASK='text-generation', or choose a provider that supports the task. | Error: (Request ID: Root=1-69879cd7-22e8e4a063dfa75d1feab252;2e15c86d-b154-4663-b3fc-a25ef71ddcf8)

Bad request:
{'message': "The requested model 'mistralai/Mistral-7B-Instruct-v0.3' is not a chat model.", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'} | File: D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\llm.py | Line: 72
2026-02-08 02:13:13,201 - INFO - 127.0.0.1 - - [08/Feb/2026 02:13:13] "POST / HTTP/1.1" 200 -
2026-02-08 02:13:28,424 - INFO - Loading vector store for context
2026-02-08 02:13:28,424 - INFO - Initializing HuggingFaceEmbeddings model...
2026-02-08 02:13:28,426 - INFO - Use pytorch device_name: cpu
2026-02-08 02:13:28,426 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2026-02-08 02:13:28,948 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:13:28,961 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-08 02:13:29,216 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:13:29,229 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-08 02:13:29,475 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:13:29,495 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-08 02:13:29,754 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:13:29,769 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md "HTTP/1.1 200 OK"
2026-02-08 02:13:30,101 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:13:30,114 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-08 02:13:30,365 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:13:30,393 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json "HTTP/1.1 200 OK"
2026-02-08 02:13:30,641 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json "HTTP/1.1 404 Not Found"
2026-02-08 02:13:30,895 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:13:30,908 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-08 02:13:31,292 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:13:31,307 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-08 02:13:31,555 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:13:31,572 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json "HTTP/1.1 200 OK"
2026-02-08 02:13:31,829 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false "HTTP/1.1 404 Not Found"
2026-02-08 02:13:32,088 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=true&expand=false "HTTP/1.1 200 OK"
2026-02-08 02:13:32,409 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:13:32,426 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json "HTTP/1.1 200 OK"
2026-02-08 02:13:32,688 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2 "HTTP/1.1 200 OK"
2026-02-08 02:13:32,692 - INFO - HuggingFaceEmbeddings model initialized successfully.
2026-02-08 02:13:32,693 - INFO - Loading existing FAISS vector store...
2026-02-08 02:13:32,695 - INFO - Loading faiss with AVX2 support.
2026-02-08 02:13:32,716 - INFO - Successfully loaded faiss with AVX2 support.
2026-02-08 02:13:32,756 - INFO - Using HF model repo id: mistralai/Mistral-7B-Instruct-v0.3
2026-02-08 02:13:32,756 - INFO - HF token present: True
2026-02-08 02:13:32,756 - INFO - Loading LLM from HuggingFace
2026-02-08 02:13:33,111 - INFO - HTTP Request: GET https://huggingface.co/api/models/mistralai/Mistral-7B-Instruct-v0.3?expand=inferenceProviderMapping "HTTP/1.1 200 OK"
2026-02-08 02:13:33,112 - WARNING - text-generation probe failed; trying conversational. Reason: Model mistralai/Mistral-7B-Instruct-v0.3 is not supported for task text-generation and provider together. Supported task: conversational.
2026-02-08 02:13:33,482 - INFO - HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 400 Bad Request"
2026-02-08 02:13:33,482 - ERROR - Failed to load a llm | Error: Hugging Face model/provider does not support either text-generation or conversational tasks. Try a different model, set HF_LLM_TASK='text-generation', or choose a provider that supports the task. | Error: (Request ID: Root=1-69879cec-6f55d3372aff32a67acb95e0;ed2e6eef-2b8f-4afc-8326-219687e471ff)

Bad request:
{'message': "The requested model 'mistralai/Mistral-7B-Instruct-v0.3' is not a chat model.", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'} | File: D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\llm.py | Line: 72 | File: D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\llm.py | Line: 77
2026-02-08 02:13:33,483 - ERROR - Failed to make a QA chain
Traceback (most recent call last):
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\llm.py", line 63, in load_llm
    probe_text(llm)
    ~~~~~~~~~~^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\llm.py", line 44, in probe_text
    _ = text_llm.invoke("ping")
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 378, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 789, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 1011, in generate
    return self._generate_helper(
           ~~~~~~~~~~~~~~~~~~~~~^
        prompts,
        ^^^^^^^^
    ...<3 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 815, in _generate_helper
    self._generate(
    ~~~~~~~~~~~~~~^
        prompts,
        ^^^^^^^^
    ...<3 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 1505, in _generate
    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_huggingface\llms\huggingface_endpoint.py", line 341, in _call
    response_text = self.client.text_generation(
        prompt=prompt,
        model=self.model,
        **invocation_params,
    )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_client.py", line 2387, in text_generation
    request_parameters = provider_helper.prepare_request(
        inputs=prompt,
    ...<4 lines>...
        api_key=self.token,
    )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_providers\_common.py", line 95, in prepare_request
    provider_mapping_info = self._prepare_mapping_info(model)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_providers\_common.py", line 173, in _prepare_mapping_info
    raise ValueError(
    ...<2 lines>...
    )
ValueError: Model mistralai/Mistral-7B-Instruct-v0.3 is not supported for task text-generation and provider together. Supported task: conversational.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\retriever.py", line 42, in create_qa_chain
    llm = load_llm(huggingface_repo_id=HUGGINGFACE_REPO_ID , hf_token=HF_TOKEN )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\llm.py", line 77, in load_llm
    raise CustomException(
    ...<3 lines>...
    ) from text_err
app.common.custom_exception.CustomException: Hugging Face model/provider does not support either text-generation or conversational tasks. Try a different model, set HF_LLM_TASK='text-generation', or choose a provider that supports the task. | Error: (Request ID: Root=1-69879cec-6f55d3372aff32a67acb95e0;ed2e6eef-2b8f-4afc-8326-219687e471ff)

Bad request:
{'message': "The requested model 'mistralai/Mistral-7B-Instruct-v0.3' is not a chat model.", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'} | File: D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\llm.py | Line: 72
2026-02-08 02:16:38,695 - INFO - Loading vector store for context
2026-02-08 02:16:38,695 - INFO - Initializing HuggingFaceEmbeddings model...
2026-02-08 02:16:38,696 - INFO - Use pytorch device_name: cpu
2026-02-08 02:16:38,696 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2026-02-08 02:16:39,229 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:16:39,242 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-08 02:16:39,492 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:16:39,504 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-08 02:16:39,757 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:16:39,776 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-08 02:16:40,296 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:16:40,321 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md "HTTP/1.1 200 OK"
2026-02-08 02:16:40,576 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:16:40,589 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-08 02:16:40,843 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:16:40,856 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json "HTTP/1.1 200 OK"
2026-02-08 02:16:41,119 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json "HTTP/1.1 404 Not Found"
2026-02-08 02:16:41,369 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:16:41,384 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-08 02:16:41,779 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:16:41,792 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-08 02:16:42,045 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:16:42,061 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json "HTTP/1.1 200 OK"
2026-02-08 02:16:42,319 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false "HTTP/1.1 404 Not Found"
2026-02-08 02:16:42,585 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=true&expand=false "HTTP/1.1 200 OK"
2026-02-08 02:16:42,942 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:16:42,971 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json "HTTP/1.1 200 OK"
2026-02-08 02:16:43,239 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2 "HTTP/1.1 200 OK"
2026-02-08 02:16:43,243 - INFO - HuggingFaceEmbeddings model initialized successfully.
2026-02-08 02:16:43,244 - INFO - Loading existing FAISS vector store...
2026-02-08 02:16:43,246 - INFO - Loading faiss with AVX2 support.
2026-02-08 02:16:43,265 - INFO - Successfully loaded faiss with AVX2 support.
2026-02-08 02:16:43,314 - INFO - Using HF model repo id: mistralai/Mistral-7B-Instruct-v0.3
2026-02-08 02:16:43,314 - INFO - HF token present: True
2026-02-08 02:16:43,314 - INFO - Loading LLM from HuggingFace
2026-02-08 02:16:43,663 - INFO - HTTP Request: GET https://huggingface.co/api/models/mistralai/Mistral-7B-Instruct-v0.3?expand=inferenceProviderMapping "HTTP/1.1 200 OK"
2026-02-08 02:16:43,663 - WARNING - text-generation probe failed; trying conversational. Reason: Model mistralai/Mistral-7B-Instruct-v0.3 is not supported for task text-generation and provider together. Supported task: conversational.
2026-02-08 02:16:43,955 - INFO - HTTP Request: POST https://router.huggingface.co/v1/chat/completions "HTTP/1.1 400 Bad Request"
2026-02-08 02:16:43,956 - ERROR - Failed to load a llm | Error: Hugging Face model/provider does not support either text-generation or conversational tasks. Try a different model, set HF_LLM_TASK='text-generation', or choose a provider that supports the task. | Error: (Request ID: Root=1-69879daa-3e33ac63630a032953598f1c;76b268a8-fae0-4c2e-ad8b-d12a6418e2a2)

Bad request:
{'message': "The requested model 'mistralai/Mistral-7B-Instruct-v0.3' is not a chat model.", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'} | File: D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\llm.py | Line: 72 | File: D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\llm.py | Line: 77
2026-02-08 02:16:43,956 - ERROR - Failed to make a QA chain
Traceback (most recent call last):
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\llm.py", line 63, in load_llm
    probe_text(llm)
    ~~~~~~~~~~^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\llm.py", line 44, in probe_text
    _ = text_llm.invoke("ping")
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 378, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 789, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 1011, in generate
    return self._generate_helper(
           ~~~~~~~~~~~~~~~~~~~~~^
        prompts,
        ^^^^^^^^
    ...<3 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 815, in _generate_helper
    self._generate(
    ~~~~~~~~~~~~~~^
        prompts,
        ^^^^^^^^
    ...<3 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 1505, in _generate
    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_huggingface\llms\huggingface_endpoint.py", line 341, in _call
    response_text = self.client.text_generation(
        prompt=prompt,
        model=self.model,
        **invocation_params,
    )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_client.py", line 2387, in text_generation
    request_parameters = provider_helper.prepare_request(
        inputs=prompt,
    ...<4 lines>...
        api_key=self.token,
    )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_providers\_common.py", line 95, in prepare_request
    provider_mapping_info = self._prepare_mapping_info(model)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_providers\_common.py", line 173, in _prepare_mapping_info
    raise ValueError(
    ...<2 lines>...
    )
ValueError: Model mistralai/Mistral-7B-Instruct-v0.3 is not supported for task text-generation and provider together. Supported task: conversational.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\retriever.py", line 42, in create_qa_chain
    llm = load_llm(huggingface_repo_id=HUGGINGFACE_REPO_ID , hf_token=HF_TOKEN )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\llm.py", line 77, in load_llm
    raise CustomException(
    ...<3 lines>...
    ) from text_err
app.common.custom_exception.CustomException: Hugging Face model/provider does not support either text-generation or conversational tasks. Try a different model, set HF_LLM_TASK='text-generation', or choose a provider that supports the task. | Error: (Request ID: Root=1-69879daa-3e33ac63630a032953598f1c;76b268a8-fae0-4c2e-ad8b-d12a6418e2a2)

Bad request:
{'message': "The requested model 'mistralai/Mistral-7B-Instruct-v0.3' is not a chat model.", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_supported'} | File: D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\llm.py | Line: 72
2026-02-08 02:19:37,927 - INFO - Loading vector store for context
2026-02-08 02:19:37,927 - INFO - Initializing HuggingFaceEmbeddings model...
2026-02-08 02:19:37,928 - INFO - Use pytorch device_name: cpu
2026-02-08 02:19:37,928 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2026-02-08 02:19:38,529 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:19:38,543 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-08 02:19:38,798 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:19:38,819 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-08 02:19:39,077 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:19:39,089 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-08 02:19:39,349 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:19:39,362 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md "HTTP/1.1 200 OK"
2026-02-08 02:19:39,611 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:19:39,625 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-08 02:19:39,877 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:19:39,894 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json "HTTP/1.1 200 OK"
2026-02-08 02:19:40,142 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json "HTTP/1.1 404 Not Found"
2026-02-08 02:19:40,398 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:19:40,411 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-08 02:19:40,811 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:19:40,825 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-08 02:19:41,086 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:19:41,100 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json "HTTP/1.1 200 OK"
2026-02-08 02:19:41,359 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false "HTTP/1.1 404 Not Found"
2026-02-08 02:19:41,617 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=true&expand=false "HTTP/1.1 200 OK"
2026-02-08 02:19:41,945 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:19:41,957 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json "HTTP/1.1 200 OK"
2026-02-08 02:19:42,428 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2 "HTTP/1.1 200 OK"
2026-02-08 02:19:42,432 - INFO - HuggingFaceEmbeddings model initialized successfully.
2026-02-08 02:19:42,432 - INFO - Loading existing FAISS vector store...
2026-02-08 02:19:42,435 - INFO - Loading faiss with AVX2 support.
2026-02-08 02:19:42,453 - INFO - Successfully loaded faiss with AVX2 support.
2026-02-08 02:19:42,501 - INFO - Using HF model repo id: mistralai/Mistral-7B-Instruct-v0.3
2026-02-08 02:19:42,501 - INFO - HF token present: True
2026-02-08 02:19:42,501 - INFO - Loading LLM from HuggingFace
2026-02-08 02:19:42,865 - INFO - HTTP Request: GET https://huggingface.co/api/models/mistralai/Mistral-7B-Instruct-v0.3 "HTTP/1.1 200 OK"
2026-02-08 02:19:42,866 - WARNING - text-generation probe failed for provider=hf-inference; Reason: Model 'mistralai/Mistral-7B-Instruct-v0.3' doesn't support task 'text-generation'. Supported tasks: 'None', got: 'text-generation'
2026-02-08 02:19:43,135 - INFO - HTTP Request: GET https://huggingface.co/api/models/mistralai/Mistral-7B-Instruct-v0.3 "HTTP/1.1 200 OK"
2026-02-08 02:19:43,136 - WARNING - conversational probe failed for provider=hf-inference; Reason: Model 'mistralai/Mistral-7B-Instruct-v0.3' doesn't support task 'conversational'. Supported tasks: 'None', got: 'conversational'
2026-02-08 02:19:43,136 - ERROR - Failed to load a llm | Error: Could not initialize Hugging Face LLM with any supported task/provider combination. Try setting HF_PROVIDER (e.g., 'hf-inference') and/or HF_LLM_TASK ('text-generation' or 'conversational'), or switch to a model supported by your chosen provider. | Error: Model 'mistralai/Mistral-7B-Instruct-v0.3' doesn't support task 'text-generation'. Supported tasks: 'None', got: 'text-generation' | File: Unknown File | Line: Unknown Line | File: D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\llm.py | Line: 90
2026-02-08 02:19:43,136 - ERROR - Failed to make a QA chain
Traceback (most recent call last):
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\retriever.py", line 42, in create_qa_chain
    llm = load_llm(huggingface_repo_id=HUGGINGFACE_REPO_ID , hf_token=HF_TOKEN )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\components\llm.py", line 90, in load_llm
    raise CustomException(
    ...<4 lines>...
    )
app.common.custom_exception.CustomException: Could not initialize Hugging Face LLM with any supported task/provider combination. Try setting HF_PROVIDER (e.g., 'hf-inference') and/or HF_LLM_TASK ('text-generation' or 'conversational'), or switch to a model supported by your chosen provider. | Error: Model 'mistralai/Mistral-7B-Instruct-v0.3' doesn't support task 'text-generation'. Supported tasks: 'None', got: 'text-generation' | File: Unknown File | Line: Unknown Line
2026-02-08 02:22:24,708 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.0.100:5000
2026-02-08 02:22:24,708 - INFO - [33mPress CTRL+C to quit[0m
2026-02-08 02:22:27,950 - INFO - User input received: hi
2026-02-08 02:22:27,950 - INFO - Loading vector store for context
2026-02-08 02:22:27,950 - INFO - Initializing HuggingFaceEmbeddings model...
2026-02-08 02:22:27,951 - INFO - Use pytorch device_name: cpu
2026-02-08 02:22:27,951 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2026-02-08 02:22:28,480 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:22:28,492 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-08 02:22:28,746 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:22:28,770 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-08 02:22:29,032 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:22:29,045 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-08 02:22:29,297 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:22:29,311 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md "HTTP/1.1 200 OK"
2026-02-08 02:22:29,563 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:22:29,832 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-08 02:22:30,164 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:22:30,178 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json "HTTP/1.1 200 OK"
2026-02-08 02:22:30,431 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json "HTTP/1.1 404 Not Found"
2026-02-08 02:22:30,688 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:22:30,701 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-08 02:22:31,072 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:22:31,086 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-08 02:22:31,346 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:22:31,382 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json "HTTP/1.1 200 OK"
2026-02-08 02:22:31,862 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false "HTTP/1.1 404 Not Found"
2026-02-08 02:22:32,195 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=true&expand=false "HTTP/1.1 200 OK"
2026-02-08 02:22:32,504 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:22:32,518 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json "HTTP/1.1 200 OK"
2026-02-08 02:22:32,781 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2 "HTTP/1.1 200 OK"
2026-02-08 02:22:32,785 - INFO - HuggingFaceEmbeddings model initialized successfully.
2026-02-08 02:22:32,785 - INFO - Loading existing FAISS vector store...
2026-02-08 02:22:32,787 - INFO - Loading faiss with AVX2 support.
2026-02-08 02:22:32,810 - INFO - Successfully loaded faiss with AVX2 support.
2026-02-08 02:22:32,851 - INFO - Using HF model repo id: mistralai/Mistral-7B-Instruct-v0.3
2026-02-08 02:22:32,851 - INFO - HF token present: True
2026-02-08 02:22:32,851 - INFO - Loading LLM from HuggingFace (Inference Providers)
2026-02-08 02:22:32,960 - INFO - LLM loaded successfully.
2026-02-08 02:22:32,961 - INFO - Sucesfully created the QA chain
2026-02-08 02:22:32,962 - INFO - QAChain type: <class 'langchain_classic.chains.retrieval_qa.base.RetrievalQA'>
2026-02-08 02:22:32,962 - INFO - Input keys: ['query']
2026-02-08 02:22:32,962 - INFO - Output keys: ['result']
2026-02-08 02:22:33,247 - INFO - HTTP Request: GET https://huggingface.co/api/models/mistralai/Mistral-7B-Instruct-v0.3 "HTTP/1.1 200 OK"
2026-02-08 02:22:33,247 - ERROR - Error while running QA chain
Traceback (most recent call last):
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\applications.py", line 41, in index
    response = qa_chain.invoke({"query" : user_input})
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 167, in invoke
    self._call(inputs, run_manager=run_manager)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\retrieval_qa\base.py", line 154, in _call
    answer = self.combine_documents_chain.run(
        input_documents=docs,
        question=question,
        callbacks=_run_manager.get_child(),
    )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\_api\deprecation.py", line 205, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 637, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\_api\deprecation.py", line 205, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 413, in __call__
    return self.invoke(
           ~~~~~~~~~~~^
        inputs,
        ^^^^^^^
    ...<2 lines>...
        include_run_info=include_run_info,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 167, in invoke
    self._call(inputs, run_manager=run_manager)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\combine_documents\base.py", line 141, in _call
    output, extra_return_dict = self.combine_docs(
                                ~~~~~~~~~~~~~~~~~^
        docs,
        ^^^^^
        callbacks=_run_manager.get_child(),
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        **other_keys,
        ^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\combine_documents\stuff.py", line 266, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\llm.py", line 315, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\_api\deprecation.py", line 205, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 413, in __call__
    return self.invoke(
           ~~~~~~~~~~~^
        inputs,
        ^^^^^^^
    ...<2 lines>...
        include_run_info=include_run_info,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 167, in invoke
    self._call(inputs, run_manager=run_manager)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\llm.py", line 117, in _call
    response = self.generate([inputs], run_manager=run_manager)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\llm.py", line 129, in generate
    return self.llm.generate_prompt(
           ~~~~~~~~~~~~~~~~~~~~~~~~^
        prompts,
        ^^^^^^^^
    ...<2 lines>...
        **self.llm_kwargs,
        ^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\chat_models.py", line 931, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_huggingface\chat_models\huggingface.py", line 750, in _generate
    answer = self.llm.client.chat_completion(messages=message_dicts, **params)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_client.py", line 926, in chat_completion
    request_parameters = provider_helper.prepare_request(
        inputs=messages,
    ...<3 lines>...
        api_key=self.token,
    )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_providers\_common.py", line 95, in prepare_request
    provider_mapping_info = self._prepare_mapping_info(model)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_providers\hf_inference.py", line 45, in _prepare_mapping_info
    _check_supported_task(model_id, self.task)
    ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_providers\hf_inference.py", line 203, in _check_supported_task
    raise ValueError(
        f"Model '{model}' doesn't support task '{task}'. Supported tasks: '{pipeline_tag}', got: '{task}'"
    )
ValueError: Model 'mistralai/Mistral-7B-Instruct-v0.3' doesn't support task 'conversational'. Supported tasks: 'None', got: 'conversational'
2026-02-08 02:22:33,276 - INFO - 127.0.0.1 - - [08/Feb/2026 02:22:33] "POST / HTTP/1.1" 200 -
2026-02-08 02:26:09,091 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.0.100:5000
2026-02-08 02:26:09,091 - INFO - [33mPress CTRL+C to quit[0m
2026-02-08 02:26:22,920 - INFO - User input received: hi
2026-02-08 02:26:22,920 - INFO - Loading vector store for context
2026-02-08 02:26:22,920 - INFO - Initializing HuggingFaceEmbeddings model...
2026-02-08 02:26:22,922 - INFO - Use pytorch device_name: cpu
2026-02-08 02:26:22,922 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2026-02-08 02:26:23,478 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:26:23,492 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-08 02:26:23,744 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:26:23,757 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-08 02:26:24,019 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:26:24,038 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-08 02:26:24,288 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:26:24,300 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md "HTTP/1.1 200 OK"
2026-02-08 02:26:24,546 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:26:24,564 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-08 02:26:24,816 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:26:24,833 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json "HTTP/1.1 200 OK"
2026-02-08 02:26:25,086 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json "HTTP/1.1 404 Not Found"
2026-02-08 02:26:25,343 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:26:25,357 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-08 02:26:25,749 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:26:25,767 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-08 02:26:26,023 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:26:26,036 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json "HTTP/1.1 200 OK"
2026-02-08 02:26:26,309 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false "HTTP/1.1 404 Not Found"
2026-02-08 02:26:26,578 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=true&expand=false "HTTP/1.1 200 OK"
2026-02-08 02:26:26,887 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:26:26,906 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json "HTTP/1.1 200 OK"
2026-02-08 02:26:27,171 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2 "HTTP/1.1 200 OK"
2026-02-08 02:26:27,178 - INFO - HuggingFaceEmbeddings model initialized successfully.
2026-02-08 02:26:27,178 - INFO - Loading existing FAISS vector store...
2026-02-08 02:26:27,193 - INFO - Loading faiss with AVX2 support.
2026-02-08 02:26:27,280 - INFO - Successfully loaded faiss with AVX2 support.
2026-02-08 02:26:27,433 - INFO - Using HF model repo id: Qwen/Qwen3-Coder-Next
2026-02-08 02:26:27,433 - INFO - HF token present: True
2026-02-08 02:26:27,433 - INFO - Loading LLM from HuggingFace
2026-02-08 02:26:27,616 - INFO - LLM loaded sucesfully...
2026-02-08 02:26:27,616 - INFO - Sucesfully created the QA chain
2026-02-08 02:26:27,616 - INFO - QAChain type: <class 'langchain_classic.chains.retrieval_qa.base.RetrievalQA'>
2026-02-08 02:26:27,616 - INFO - Input keys: ['query']
2026-02-08 02:26:27,616 - INFO - Output keys: ['result']
2026-02-08 02:26:27,901 - INFO - HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen3-Coder-Next?expand=inferenceProviderMapping "HTTP/1.1 200 OK"
2026-02-08 02:26:27,901 - ERROR - Error while running QA chain
Traceback (most recent call last):
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\applications.py", line 41, in index
    response = qa_chain.invoke({"query" : user_input})
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 167, in invoke
    self._call(inputs, run_manager=run_manager)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\retrieval_qa\base.py", line 154, in _call
    answer = self.combine_documents_chain.run(
        input_documents=docs,
        question=question,
        callbacks=_run_manager.get_child(),
    )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\_api\deprecation.py", line 205, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 637, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\_api\deprecation.py", line 205, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 413, in __call__
    return self.invoke(
           ~~~~~~~~~~~^
        inputs,
        ^^^^^^^
    ...<2 lines>...
        include_run_info=include_run_info,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 167, in invoke
    self._call(inputs, run_manager=run_manager)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\combine_documents\base.py", line 141, in _call
    output, extra_return_dict = self.combine_docs(
                                ~~~~~~~~~~~~~~~~~^
        docs,
        ^^^^^
        callbacks=_run_manager.get_child(),
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        **other_keys,
        ^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\combine_documents\stuff.py", line 266, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\llm.py", line 315, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\_api\deprecation.py", line 205, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 413, in __call__
    return self.invoke(
           ~~~~~~~~~~~^
        inputs,
        ^^^^^^^
    ...<2 lines>...
        include_run_info=include_run_info,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 167, in invoke
    self._call(inputs, run_manager=run_manager)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\llm.py", line 117, in _call
    response = self.generate([inputs], run_manager=run_manager)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\llm.py", line 129, in generate
    return self.llm.generate_prompt(
           ~~~~~~~~~~~~~~~~~~~~~~~~^
        prompts,
        ^^^^^^^^
    ...<2 lines>...
        **self.llm_kwargs,
        ^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 789, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 1011, in generate
    return self._generate_helper(
           ~~~~~~~~~~~~~~~~~~~~~^
        prompts,
        ^^^^^^^^
    ...<3 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 815, in _generate_helper
    self._generate(
    ~~~~~~~~~~~~~~^
        prompts,
        ^^^^^^^^
    ...<3 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\llms.py", line 1505, in _generate
    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_huggingface\llms\huggingface_endpoint.py", line 341, in _call
    response_text = self.client.text_generation(
        prompt=prompt,
        model=self.model,
        **invocation_params,
    )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_client.py", line 2387, in text_generation
    request_parameters = provider_helper.prepare_request(
        inputs=prompt,
    ...<4 lines>...
        api_key=self.token,
    )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_providers\_common.py", line 95, in prepare_request
    provider_mapping_info = self._prepare_mapping_info(model)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_providers\_common.py", line 173, in _prepare_mapping_info
    raise ValueError(
    ...<2 lines>...
    )
ValueError: Model Qwen/Qwen3-Coder-Next is not supported for task text-generation and provider novita. Supported task: conversational.
2026-02-08 02:26:28,036 - INFO - 127.0.0.1 - - [08/Feb/2026 02:26:28] "POST / HTTP/1.1" 200 -
2026-02-08 02:27:01,105 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.0.100:5000
2026-02-08 02:27:01,105 - INFO - [33mPress CTRL+C to quit[0m
2026-02-08 02:27:08,487 - INFO - User input received: hi
2026-02-08 02:27:08,487 - INFO - Loading vector store for context
2026-02-08 02:27:08,487 - INFO - Initializing HuggingFaceEmbeddings model...
2026-02-08 02:27:08,488 - INFO - Use pytorch device_name: cpu
2026-02-08 02:27:08,488 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2026-02-08 02:27:09,000 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:27:09,012 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-08 02:27:09,268 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:27:09,282 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-08 02:27:09,548 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:27:09,575 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-08 02:27:09,829 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:27:09,841 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md "HTTP/1.1 200 OK"
2026-02-08 02:27:10,090 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:27:10,108 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-08 02:27:10,355 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:27:10,368 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json "HTTP/1.1 200 OK"
2026-02-08 02:27:10,857 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json "HTTP/1.1 404 Not Found"
2026-02-08 02:27:11,108 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:27:11,124 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-08 02:27:11,496 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:27:11,510 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-08 02:27:11,756 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:27:11,773 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json "HTTP/1.1 200 OK"
2026-02-08 02:27:12,044 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false "HTTP/1.1 404 Not Found"
2026-02-08 02:27:12,307 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=true&expand=false "HTTP/1.1 200 OK"
2026-02-08 02:27:12,627 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:27:12,640 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json "HTTP/1.1 200 OK"
2026-02-08 02:27:35,560 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2 "HTTP/1.1 200 OK"
2026-02-08 02:27:35,572 - INFO - HuggingFaceEmbeddings model initialized successfully.
2026-02-08 02:27:35,572 - INFO - Loading existing FAISS vector store...
2026-02-08 02:27:35,580 - INFO - Loading faiss with AVX2 support.
2026-02-08 02:27:35,601 - INFO - Successfully loaded faiss with AVX2 support.
2026-02-08 02:27:35,639 - INFO - Using HF model repo id: Qwen/Qwen3-Coder-Next
2026-02-08 02:27:35,639 - INFO - HF token present: True
2026-02-08 02:27:35,639 - INFO - Loading LLM from HuggingFace (Inference Providers)
2026-02-08 02:27:35,835 - INFO - LLM loaded successfully.
2026-02-08 02:27:35,836 - INFO - Sucesfully created the QA chain
2026-02-08 02:27:35,836 - INFO - QAChain type: <class 'langchain_classic.chains.retrieval_qa.base.RetrievalQA'>
2026-02-08 02:27:35,836 - INFO - Input keys: ['query']
2026-02-08 02:27:35,836 - INFO - Output keys: ['result']
2026-02-08 02:27:36,132 - INFO - HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen3-Coder-Next "HTTP/1.1 200 OK"
2026-02-08 02:27:36,493 - INFO - HTTP Request: POST https://router.huggingface.co/hf-inference/models/Qwen/Qwen3-Coder-Next/v1/chat/completions "HTTP/1.1 404 Not Found"
2026-02-08 02:27:36,493 - ERROR - Error while running QA chain
Traceback (most recent call last):
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\utils\_http.py", line 657, in hf_raise_for_status
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'https://router.huggingface.co/hf-inference/models/Qwen/Qwen3-Coder-Next/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\applications.py", line 41, in index
    response = qa_chain.invoke({"query" : user_input})
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 167, in invoke
    self._call(inputs, run_manager=run_manager)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\retrieval_qa\base.py", line 154, in _call
    answer = self.combine_documents_chain.run(
        input_documents=docs,
        question=question,
        callbacks=_run_manager.get_child(),
    )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\_api\deprecation.py", line 205, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 637, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\_api\deprecation.py", line 205, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 413, in __call__
    return self.invoke(
           ~~~~~~~~~~~^
        inputs,
        ^^^^^^^
    ...<2 lines>...
        include_run_info=include_run_info,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 167, in invoke
    self._call(inputs, run_manager=run_manager)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\combine_documents\base.py", line 141, in _call
    output, extra_return_dict = self.combine_docs(
                                ~~~~~~~~~~~~~~~~~^
        docs,
        ^^^^^
        callbacks=_run_manager.get_child(),
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        **other_keys,
        ^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\combine_documents\stuff.py", line 266, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\llm.py", line 315, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\_api\deprecation.py", line 205, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 413, in __call__
    return self.invoke(
           ~~~~~~~~~~~^
        inputs,
        ^^^^^^^
    ...<2 lines>...
        include_run_info=include_run_info,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 167, in invoke
    self._call(inputs, run_manager=run_manager)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\llm.py", line 117, in _call
    response = self.generate([inputs], run_manager=run_manager)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\llm.py", line 129, in generate
    return self.llm.generate_prompt(
           ~~~~~~~~~~~~~~~~~~~~~~~~^
        prompts,
        ^^^^^^^^
    ...<2 lines>...
        **self.llm_kwargs,
        ^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\chat_models.py", line 931, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_huggingface\chat_models\huggingface.py", line 750, in _generate
    answer = self.llm.client.chat_completion(messages=message_dicts, **params)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_client.py", line 933, in chat_completion
    data = self._inner_post(request_parameters, stream=stream)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_client.py", line 286, in _inner_post
    hf_raise_for_status(response)
    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\utils\_http.py", line 752, in hf_raise_for_status
    raise _format(HfHubHTTPError, str(e), response) from e
huggingface_hub.errors.HfHubHTTPError: Client error '404 Not Found' for url 'https://router.huggingface.co/hf-inference/models/Qwen/Qwen3-Coder-Next/v1/chat/completions' (Request ID: Root=1-6987a037-612c6d930e711a1e737582a3;07a9d224-d4f7-491c-86f4-db840ca6dd07)
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
2026-02-08 02:27:36,532 - INFO - 127.0.0.1 - - [08/Feb/2026 02:27:36] "POST / HTTP/1.1" 200 -
2026-02-08 02:28:38,052 - INFO - 127.0.0.1 - - [08/Feb/2026 02:28:38] "[32mGET /clear HTTP/1.1[0m" 302 -
2026-02-08 02:28:38,057 - INFO - 127.0.0.1 - - [08/Feb/2026 02:28:38] "GET / HTTP/1.1" 200 -
2026-02-08 02:28:39,242 - INFO - User input received: hi
2026-02-08 02:28:39,242 - INFO - Loading vector store for context
2026-02-08 02:28:39,242 - INFO - Initializing HuggingFaceEmbeddings model...
2026-02-08 02:28:39,244 - INFO - Use pytorch device_name: cpu
2026-02-08 02:28:39,244 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2026-02-08 02:28:39,516 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:28:39,529 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-08 02:28:39,789 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:28:39,802 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-08 02:28:40,055 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:28:40,068 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-08 02:28:40,317 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:28:40,335 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md "HTTP/1.1 200 OK"
2026-02-08 02:28:40,588 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:28:40,601 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-08 02:28:40,858 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:28:40,871 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json "HTTP/1.1 200 OK"
2026-02-08 02:28:41,120 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json "HTTP/1.1 404 Not Found"
2026-02-08 02:28:41,369 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:28:41,382 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-08 02:28:41,705 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:28:41,718 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-08 02:28:41,977 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:28:41,989 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json "HTTP/1.1 200 OK"
2026-02-08 02:28:42,245 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false "HTTP/1.1 404 Not Found"
2026-02-08 02:28:42,507 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=true&expand=false "HTTP/1.1 200 OK"
2026-02-08 02:28:42,817 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:28:42,833 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json "HTTP/1.1 200 OK"
2026-02-08 02:28:43,102 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2 "HTTP/1.1 200 OK"
2026-02-08 02:28:43,104 - INFO - HuggingFaceEmbeddings model initialized successfully.
2026-02-08 02:28:43,104 - INFO - Loading existing FAISS vector store...
2026-02-08 02:28:43,276 - INFO - Using HF model repo id: Qwen/Qwen3-Coder-Next
2026-02-08 02:28:43,276 - INFO - HF token present: True
2026-02-08 02:28:43,276 - INFO - Loading LLM from HuggingFace (Inference Providers)
2026-02-08 02:28:43,276 - INFO - LLM loaded successfully.
2026-02-08 02:28:43,277 - INFO - Sucesfully created the QA chain
2026-02-08 02:28:43,277 - INFO - QAChain type: <class 'langchain_classic.chains.retrieval_qa.base.RetrievalQA'>
2026-02-08 02:28:43,277 - INFO - Input keys: ['query']
2026-02-08 02:28:43,277 - INFO - Output keys: ['result']
2026-02-08 02:28:43,643 - INFO - HTTP Request: POST https://router.huggingface.co/hf-inference/models/Qwen/Qwen3-Coder-Next/v1/chat/completions "HTTP/1.1 404 Not Found"
2026-02-08 02:28:43,644 - ERROR - Error while running QA chain
Traceback (most recent call last):
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\utils\_http.py", line 657, in hf_raise_for_status
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'https://router.huggingface.co/hf-inference/models/Qwen/Qwen3-Coder-Next/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\app\applications.py", line 41, in index
    response = qa_chain.invoke({"query" : user_input})
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 167, in invoke
    self._call(inputs, run_manager=run_manager)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\retrieval_qa\base.py", line 154, in _call
    answer = self.combine_documents_chain.run(
        input_documents=docs,
        question=question,
        callbacks=_run_manager.get_child(),
    )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\_api\deprecation.py", line 205, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 637, in run
    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[
           ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\_api\deprecation.py", line 205, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 413, in __call__
    return self.invoke(
           ~~~~~~~~~~~^
        inputs,
        ^^^^^^^
    ...<2 lines>...
        include_run_info=include_run_info,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 167, in invoke
    self._call(inputs, run_manager=run_manager)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\combine_documents\base.py", line 141, in _call
    output, extra_return_dict = self.combine_docs(
                                ~~~~~~~~~~~~~~~~~^
        docs,
        ^^^^^
        callbacks=_run_manager.get_child(),
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        **other_keys,
        ^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\combine_documents\stuff.py", line 266, in combine_docs
    return self.llm_chain.predict(callbacks=callbacks, **inputs), {}
           ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\llm.py", line 315, in predict
    return self(kwargs, callbacks=callbacks)[self.output_key]
           ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\_api\deprecation.py", line 205, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 413, in __call__
    return self.invoke(
           ~~~~~~~~~~~^
        inputs,
        ^^^^^^^
    ...<2 lines>...
        include_run_info=include_run_info,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\base.py", line 167, in invoke
    self._call(inputs, run_manager=run_manager)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\llm.py", line 117, in _call
    response = self.generate([inputs], run_manager=run_manager)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_classic\chains\llm.py", line 129, in generate
    return self.llm.generate_prompt(
           ~~~~~~~~~~~~~~~~~~~~~~~~^
        prompts,
        ^^^^^^^^
    ...<2 lines>...
        **self.llm_kwargs,
        ^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1121, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\chat_models.py", line 931, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1233, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\langchain_huggingface\chat_models\huggingface.py", line 750, in _generate
    answer = self.llm.client.chat_completion(messages=message_dicts, **params)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_client.py", line 933, in chat_completion
    data = self._inner_post(request_parameters, stream=stream)
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\inference\_client.py", line 286, in _inner_post
    hf_raise_for_status(response)
    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "D:\1MAHIR\mlops\projects\RAG Medical Chatbot\rag\Lib\site-packages\huggingface_hub\utils\_http.py", line 752, in hf_raise_for_status
    raise _format(HfHubHTTPError, str(e), response) from e
huggingface_hub.errors.HfHubHTTPError: Client error '404 Not Found' for url 'https://router.huggingface.co/hf-inference/models/Qwen/Qwen3-Coder-Next/v1/chat/completions' (Request ID: Root=1-6987a07a-28ff0253100ed2484a49a379;fdfd96b5-0544-499e-8bf2-c8f7eda33c92)
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
2026-02-08 02:28:43,652 - INFO - 127.0.0.1 - - [08/Feb/2026 02:28:43] "POST / HTTP/1.1" 200 -
2026-02-08 02:29:30,554 - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://192.168.0.100:5000
2026-02-08 02:29:30,554 - INFO - [33mPress CTRL+C to quit[0m
2026-02-08 02:29:36,111 - INFO - User input received: hi
2026-02-08 02:29:36,111 - INFO - Loading vector store for context
2026-02-08 02:29:36,111 - INFO - Initializing HuggingFaceEmbeddings model...
2026-02-08 02:29:36,113 - INFO - Use pytorch device_name: cpu
2026-02-08 02:29:36,113 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2026-02-08 02:29:36,648 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:29:36,661 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-08 02:29:36,909 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:29:36,922 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-08 02:29:37,181 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:29:37,195 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-08 02:29:37,445 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:29:37,457 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md "HTTP/1.1 200 OK"
2026-02-08 02:29:37,702 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:29:37,721 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-08 02:29:37,971 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:29:37,984 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json "HTTP/1.1 200 OK"
2026-02-08 02:29:38,250 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json "HTTP/1.1 404 Not Found"
2026-02-08 02:29:38,546 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:29:38,559 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-08 02:29:38,942 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:29:38,961 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-08 02:29:39,258 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:29:39,271 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json "HTTP/1.1 200 OK"
2026-02-08 02:29:39,522 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false "HTTP/1.1 404 Not Found"
2026-02-08 02:29:39,786 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=true&expand=false "HTTP/1.1 200 OK"
2026-02-08 02:29:40,113 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:29:40,129 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json "HTTP/1.1 200 OK"
2026-02-08 02:29:40,394 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2 "HTTP/1.1 200 OK"
2026-02-08 02:29:40,410 - INFO - HuggingFaceEmbeddings model initialized successfully.
2026-02-08 02:29:40,411 - INFO - Loading existing FAISS vector store...
2026-02-08 02:29:40,416 - INFO - Loading faiss with AVX2 support.
2026-02-08 02:29:40,445 - INFO - Successfully loaded faiss with AVX2 support.
2026-02-08 02:29:40,482 - INFO - Using HF model repo id: Qwen/Qwen3-Coder-Next
2026-02-08 02:29:40,482 - INFO - HF token present: True
2026-02-08 02:29:40,482 - INFO - Loading LLM from HuggingFace (Inference Providers)
2026-02-08 02:29:40,584 - INFO - LLM loaded successfully.
2026-02-08 02:29:40,584 - INFO - Sucesfully created the QA chain
2026-02-08 02:29:40,584 - INFO - QAChain type: <class 'langchain_classic.chains.retrieval_qa.base.RetrievalQA'>
2026-02-08 02:29:40,584 - INFO - Input keys: ['query']
2026-02-08 02:29:40,584 - INFO - Output keys: ['result']
2026-02-08 02:29:40,854 - INFO - HTTP Request: GET https://huggingface.co/api/models/Qwen/Qwen3-Coder-Next?expand=inferenceProviderMapping "HTTP/1.1 200 OK"
2026-02-08 02:29:42,140 - INFO - HTTP Request: POST https://router.huggingface.co/novita/v3/openai/chat/completions "HTTP/1.1 200 OK"
2026-02-08 02:29:42,161 - INFO - LLM response: {'query': 'hi', 'result': 'The provided context appears to be a Library of Congress Cataloging-in-Publication Data record, not medical information.  \nNo medical details are included.'}
2026-02-08 02:29:42,169 - INFO - 127.0.0.1 - - [08/Feb/2026 02:29:42] "[32mPOST / HTTP/1.1[0m" 302 -
2026-02-08 02:29:42,177 - INFO - 127.0.0.1 - - [08/Feb/2026 02:29:42] "GET / HTTP/1.1" 200 -
2026-02-08 02:29:56,746 - INFO - User input received: what is cancer
2026-02-08 02:29:56,746 - INFO - Loading vector store for context
2026-02-08 02:29:56,746 - INFO - Initializing HuggingFaceEmbeddings model...
2026-02-08 02:29:56,748 - INFO - Use pytorch device_name: cpu
2026-02-08 02:29:56,748 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2026-02-08 02:29:57,068 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:29:57,081 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-08 02:29:57,341 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:29:57,353 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-08 02:29:57,606 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:29:57,622 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json "HTTP/1.1 200 OK"
2026-02-08 02:29:57,889 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:29:57,904 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md "HTTP/1.1 200 OK"
2026-02-08 02:29:58,160 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:29:58,174 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json "HTTP/1.1 200 OK"
2026-02-08 02:29:58,421 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:29:58,436 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json "HTTP/1.1 200 OK"
2026-02-08 02:29:58,707 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json "HTTP/1.1 404 Not Found"
2026-02-08 02:29:58,962 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:29:58,976 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-08 02:29:59,360 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:29:59,376 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json "HTTP/1.1 200 OK"
2026-02-08 02:29:59,632 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:29:59,648 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json "HTTP/1.1 200 OK"
2026-02-08 02:29:59,904 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false "HTTP/1.1 404 Not Found"
2026-02-08 02:30:00,161 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=true&expand=false "HTTP/1.1 200 OK"
2026-02-08 02:30:00,474 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json "HTTP/1.1 307 Temporary Redirect"
2026-02-08 02:30:00,488 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json "HTTP/1.1 200 OK"
2026-02-08 02:30:23,805 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2 "HTTP/1.1 200 OK"
2026-02-08 02:30:23,811 - INFO - HuggingFaceEmbeddings model initialized successfully.
2026-02-08 02:30:23,811 - INFO - Loading existing FAISS vector store...
2026-02-08 02:30:23,957 - INFO - Using HF model repo id: Qwen/Qwen3-Coder-Next
2026-02-08 02:30:23,957 - INFO - HF token present: True
2026-02-08 02:30:23,958 - INFO - Loading LLM from HuggingFace (Inference Providers)
2026-02-08 02:30:23,958 - INFO - LLM loaded successfully.
2026-02-08 02:30:23,958 - INFO - Sucesfully created the QA chain
2026-02-08 02:30:23,958 - INFO - QAChain type: <class 'langchain_classic.chains.retrieval_qa.base.RetrievalQA'>
2026-02-08 02:30:23,958 - INFO - Input keys: ['query']
2026-02-08 02:30:23,958 - INFO - Output keys: ['result']
2026-02-08 02:30:25,249 - INFO - HTTP Request: POST https://router.huggingface.co/novita/v3/openai/chat/completions "HTTP/1.1 200 OK"
2026-02-08 02:30:25,251 - INFO - LLM response: {'query': 'what is cancer', 'result': 'Cancer is a disease of the genes, where mutations disrupt the normal control of cell growth and division. This leads to uncontrolled cell proliferation due to faulty proteins produced by altered genes.'}
2026-02-08 02:30:25,256 - INFO - 127.0.0.1 - - [08/Feb/2026 02:30:25] "[32mPOST / HTTP/1.1[0m" 302 -
2026-02-08 02:30:25,260 - INFO - 127.0.0.1 - - [08/Feb/2026 02:30:25] "GET / HTTP/1.1" 200 -
